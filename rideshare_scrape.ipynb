{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Initial scrape of Craigslist rideshare page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _return_soup_UScities():\n",
    "    \"\"\" Return BeautifulSoup object of rideshare page \"\"\"\n",
    "\n",
    "    url = 'https://www.craigslist.org/about/sites#US'\n",
    "    response = requests.get(url)\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page,\"html.parser\")\n",
    "    return soup\n",
    "\n",
    "def links_US_locations():\n",
    "    \"\"\" Return dictionary of US locations mapped to craigslist URL \"\"\"\n",
    "\n",
    "    soup = _return_soup_UScities()\n",
    "\n",
    "    #finds first instance of colmask tag in link pointing to #US\n",
    "    US_locations = soup.find('div',{'class':'colmask'}).findAll('a')\n",
    "    \n",
    "    return {str(item.text):str(item['href']) for item in US_locations}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load dictionary of US locations mapped to URLS on Craigslist\n",
    "US_locations = links_US_locations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class cityRideshare:\n",
    "    \n",
    "    def __init__(self, city, dict_cities_links):\n",
    "        self.city = city.lower()\n",
    "        self.dict_cities_links = dict_cities_links\n",
    "        \n",
    "        # fetch Craigslist URL from dictionary\n",
    "        self.url = self.dict_cities_links.get(self.city)\n",
    "        \n",
    "    def _return_soup(self, rid = None, page = 0):\n",
    "        \"\"\" Return BeautifulSoup object of rideshare page \"\"\"\n",
    "\n",
    "        url = 'https:{0}search/rid?s={1}00'.format(self.url,page)\n",
    "        if rid:\n",
    "            url = 'https:{0}rid/{1}.html'.format(self.url,rid)\n",
    "        response = requests.get(url)\n",
    "        page = response.text\n",
    "        soup = BeautifulSoup(page,\"html.parser\")\n",
    "        return soup\n",
    "    \n",
    "    def _scrape_one_page(self, soup): \n",
    "        \"\"\" Returns pandas DataFrame with RID, title and timestamp of each posting\"\"\"\n",
    "    \n",
    "        info = []\n",
    "        for line in soup.find_all('span',{'class':'pl'}):\n",
    "            d = {}\n",
    "            d['id'] = str(line.find('a',{'class':'hdrlnk'})['data-id'])\n",
    "            d['title'] = line.find('span',{'id':'titletextonly'}).text\n",
    "            d['timestamp'] = line.find('time')['datetime']\n",
    "            info.append(d)\n",
    "\n",
    "        return pd.DataFrame(info)\n",
    "    \n",
    "    def _scrape_posts(self, df):\n",
    "        \"\"\" Scrape each post for posting body and type of ride.\n",
    "        Add info to existing DataFrame using previously collected RIDs.\n",
    "        Return DataFrame \"\"\"\n",
    "        \n",
    "        for i,row in df.iterrows():\n",
    "            if i%10==0:\n",
    "                print \"scraping text from post {0} of {1}\".format(i,len(all_posts))\n",
    "            \n",
    "            soup_post = self._return_soup(rid=row['id'])\n",
    "            df.ix[i,'text'] = soup_post.find('section',{'id':'postingbody'}).text.replace('\\n','')\n",
    "            df.ix[i,'ride_type'] = str(soup_post.find('p',{'class':'attrgroup'}).span.text)\n",
    "            time.sleep(1) #avoid crease and desist from Craigslist\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    def scrape_all_pages(self):\n",
    "        \"\"\" Returns pandas DataFrame with information from all pages of search\"\"\"\n",
    "    \n",
    "        all_posts = pd.DataFrame()\n",
    "\n",
    "        soup = self._return_soup()\n",
    "        pages = int(soup.find('span',{'class','totalcount'}).text)/100\n",
    "\n",
    "        for page in range(0,pages+1):\n",
    "            print \"scraping page:\",page\n",
    "            soup = self._return_soup(page=page)\n",
    "            df = self._scrape_one_page(soup)\n",
    "            all_posts = all_posts.append(df)\n",
    "            time.sleep(1)\n",
    "\n",
    "        all_posts.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        all_posts = _scrape_posts(all_posts)\n",
    "\n",
    "        return all_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping page: 0\n",
      "scraping page: 1\n",
      "scraping page: 2\n",
      "scraping page: 3\n",
      "scraping text from post 0 of 359\n",
      "scraping text from post 10 of 359\n",
      "scraping text from post 20 of 359\n",
      "scraping text from post 30 of 359\n",
      "scraping text from post 40 of 359\n",
      "scraping text from post 50 of 359\n",
      "scraping text from post 60 of 359\n",
      "scraping text from post 70 of 359\n",
      "scraping text from post 80 of 359\n",
      "scraping text from post 90 of 359\n",
      "scraping text from post 100 of 359\n",
      "scraping text from post 110 of 359\n",
      "scraping text from post 120 of 359\n",
      "scraping text from post 130 of 359\n",
      "scraping text from post 140 of 359\n",
      "scraping text from post 150 of 359\n",
      "scraping text from post 160 of 359\n",
      "scraping text from post 170 of 359\n",
      "scraping text from post 180 of 359\n",
      "scraping text from post 190 of 359\n",
      "scraping text from post 200 of 359\n",
      "scraping text from post 210 of 359\n",
      "scraping text from post 220 of 359\n",
      "scraping text from post 230 of 359\n",
      "scraping text from post 240 of 359\n",
      "scraping text from post 250 of 359\n",
      "scraping text from post 260 of 359\n",
      "scraping text from post 270 of 359\n",
      "scraping text from post 280 of 359\n",
      "scraping text from post 290 of 359\n",
      "scraping text from post 300 of 359\n",
      "scraping text from post 310 of 359\n",
      "scraping text from post 320 of 359\n",
      "scraping text from post 330 of 359\n",
      "scraping text from post 340 of 359\n",
      "scraping text from post 350 of 359\n"
     ]
    }
   ],
   "source": [
    "%timeit\n",
    "df_houston = cityRideshare('houston',US_locations).scrape_all_pages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#todo: find/write code to scrape dates in all different formats\n",
    "\n",
    "#todo: detect origin and destination location\n",
    "\n",
    "#todo: scrape every x days but store past data\n",
    "\n",
    "#todo: use https://www.craigslist.org/about/sites#US to navigate to different cities. \n",
    "#auto-fill options to only show these cities\n",
    "\n",
    "#inherit from master soup class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5519837630</td>\n",
       "      <td>2016-04-11 22:23</td>\n",
       "      <td>DO YOU NEED RIDE TO ANY PART OF HOUSTON AND OT...</td>\n",
       "      <td>For trips to functions and jobs call jason. Fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5519835301</td>\n",
       "      <td>2016-04-11 22:23</td>\n",
       "      <td>DO YOU NEED RIDE TO ANY PART OF HOUSTON AND OT...</td>\n",
       "      <td>For trips to functions and jobs call jason. Fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5493065802</td>\n",
       "      <td>2016-04-11 22:23</td>\n",
       "      <td>DO YOU NEED A RIDE TO ANY PART OF HOUSTON</td>\n",
       "      <td>For trips to functions and jobs call jason. Fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5535002861</td>\n",
       "      <td>2016-04-11 21:59</td>\n",
       "      <td>Seeking ride to Dallas.</td>\n",
       "      <td>I posted once before, seeking ride to clear up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5534974217</td>\n",
       "      <td>2016-04-11 21:18</td>\n",
       "      <td>ISO Houston to Austin on 4/29 (Levitation Fest)</td>\n",
       "      <td>Yo!I will be flying into Houston around noon o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id         timestamp  \\\n",
       "0  5519837630  2016-04-11 22:23   \n",
       "1  5519835301  2016-04-11 22:23   \n",
       "2  5493065802  2016-04-11 22:23   \n",
       "3  5535002861  2016-04-11 21:59   \n",
       "4  5534974217  2016-04-11 21:18   \n",
       "\n",
       "                                               title  \\\n",
       "0  DO YOU NEED RIDE TO ANY PART OF HOUSTON AND OT...   \n",
       "1  DO YOU NEED RIDE TO ANY PART OF HOUSTON AND OT...   \n",
       "2          DO YOU NEED A RIDE TO ANY PART OF HOUSTON   \n",
       "3                            Seeking ride to Dallas.   \n",
       "4    ISO Houston to Austin on 4/29 (Levitation Fest)   \n",
       "\n",
       "                                                text  \n",
       "0  For trips to functions and jobs call jason. Fo...  \n",
       "1  For trips to functions and jobs call jason. Fo...  \n",
       "2  For trips to functions and jobs call jason. Fo...  \n",
       "3  I posted once before, seeking ride to clear up...  \n",
       "4  Yo!I will be flying into Houston around noon o...  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_houston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('houston.pkl','wb') as f:\n",
    "    pickle.dump(df_houston,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
