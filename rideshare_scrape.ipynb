{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Initial scrape of Craigslist rideshare page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _return_soup_UScities():\n",
    "    \"\"\" Return BeautifulSoup object of rideshare page \"\"\"\n",
    "\n",
    "    url = 'https://www.craigslist.org/about/sites#US'\n",
    "    response = requests.get(url)\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page,\"html.parser\")\n",
    "    return soup\n",
    "\n",
    "def links_US_locations():\n",
    "    \"\"\" Return dictionary of US locations mapped to craigslist URL \"\"\"\n",
    "\n",
    "    soup = _return_soup_UScities()\n",
    "\n",
    "    # finds first instance of colmask tag in link pointing to #US\n",
    "    US_locations = soup.find('div',{'class':'colmask'}).findAll('a')\n",
    "    \n",
    "    # set URL as dict key due to possible duplicate city names\n",
    "    return {str(item['href']):str(item.text) for item in US_locations}\n",
    "\n",
    "def choose_random_city(US_locations_dict):\n",
    "    \n",
    "    city_URL = np.random.choice(US_locations_dict.keys())\n",
    "    city = US_locations_dict.get(city_URL)\n",
    "    \n",
    "    return city_URL, city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load dictionary of US locations mapped to URLS on Craigslist\n",
    "US_locations = links_US_locations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample master database file\n",
    "with open('./data/houston.pkl', 'r') as f:\n",
    "    master_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_df['city'] = 'houston'\n",
    "master_df['url'] = '//houston.craigslist.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class cityRideshare:\n",
    "    \n",
    "    def __init__(self, US_locations_dict, master_db, url=None):\n",
    "        \n",
    "        if url:\n",
    "            self.url = url\n",
    "            self.city = US_locations_dict[url]\n",
    "        else:\n",
    "            self.url, self.city = choose_random_city(US_locations_dict)\n",
    "        \n",
    "        self.master_db = master_db\n",
    "        self.master_db_this_city = self.master_db[self.master_db['city']==self.city]\n",
    "        \n",
    "        print \"scraping: \",self.city\n",
    "    \n",
    "    def _return_soup(self, rid = None, page = 0):\n",
    "        \"\"\" Return BeautifulSoup object of rideshare page \"\"\"\n",
    "\n",
    "        url = 'https:{0}search/rid?s={1}00'.format(self.url,page)\n",
    "        if rid:\n",
    "            url = 'https:{0}rid/{1}.html'.format(self.url,rid)\n",
    "        response = requests.get(url)\n",
    "        page = response.text\n",
    "        soup = BeautifulSoup(page,\"html.parser\")\n",
    "        return soup\n",
    "    \n",
    "    def _scrape_one_page(self, soup): \n",
    "        \"\"\" Returns pandas DataFrame with RID, title and timestamp of each posting\"\"\"\n",
    "    \n",
    "        info = []\n",
    "        for line in soup.find_all('span',{'class':'pl'}):\n",
    "            d = {}\n",
    "            d['id'] = str(line.find('a',{'class':'hdrlnk'})['data-id'])\n",
    "            d['title'] = line.find('span',{'id':'titletextonly'}).text\n",
    "            d['timestamp'] = line.find('time')['datetime']\n",
    "            info.append(d)\n",
    "\n",
    "        return pd.DataFrame(info)\n",
    "    \n",
    "    def _scrape_all_pages(self):\n",
    "        \"\"\" Returns pandas DataFrame with information from all rideshare pages of URL\"\"\"\n",
    "    \n",
    "        all_posts = pd.DataFrame()\n",
    "\n",
    "        soup = self._return_soup()\n",
    "        pages = int(soup.find('span',{'class','totalcount'}).text)/100\n",
    "\n",
    "        for page in range(0,pages+1):\n",
    "            print \"scraping page:\", page\n",
    "            soup = self._return_soup(page=page)\n",
    "            df = self._scrape_one_page(soup) # scrape each page for id, timestamp, title of each ride\n",
    "            all_posts = all_posts.append(df)\n",
    "            time.sleep(1) #avoid crease and desist from Craigslist\n",
    "\n",
    "        all_posts.set_index('id', inplace=True)\n",
    "        \n",
    "        all_posts['url'] = self.url\n",
    "        all_posts['city'] = self.city\n",
    "\n",
    "        return all_posts\n",
    "\n",
    "    def _RIDS_to_maintain(self, df):\n",
    "        \"\"\" Check master dataset (to be stored in a db) for existence of RIDs \n",
    "        For now use master_db as placeholder, assume index is set to RID \"\"\"\n",
    "\n",
    "        master_db_this_city = self.master_db_this_city\n",
    "        \n",
    "        new_rids = df.index.difference(master_db_this_city.index)\n",
    "        same_rids = master_db_this_city.index.intersection(df.index)\n",
    "        #old_rids = master_db_this_city.index.difference(df.index)\n",
    "        \n",
    "        keep_rids = new_rids.union(same_rids)\n",
    "        \n",
    "        return keep_rids\n",
    "    \n",
    "    def _scrape_posting_body(self, df):\n",
    "        \"\"\" Scrape each post for posting body and type of ride.\n",
    "        Add info to existing DataFrame using previously collected RIDs.\n",
    "        Return DataFrame \"\"\"\n",
    "        \n",
    "        for n, rid in enumerate(df.index):\n",
    "            if n%10==0:\n",
    "                print \"scraping text from post {0} of {1}\".format(n, len(df))\n",
    "\n",
    "            soup_post = self._return_soup(rid=rid)\n",
    "            df.ix[rid,'text'] = soup_post.find('section',{'id':'postingbody'}).text.replace('\\n','')\n",
    "            df.ix[rid,'ride_type'] = str(soup_post.find('p',{'class':'attrgroup'}).span.text)\n",
    "            time.sleep(1) #avoid crease and desist from Craigslist\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    def update_rides(self):\n",
    "        \n",
    "        rides_without_body = self._scrape_all_pages()\n",
    "        \n",
    "        RIDS_to_maintain = self._RIDS_to_maintain(rides_without_body)\n",
    "        \n",
    "        print \"original number of rides:\", len(self.master_db_this_city)\n",
    "        print \"updated number of rides:\", len(RIDS_to_maintain)\n",
    "        \n",
    "        rides_with_body = self._scrape_posting_body(rides_without_body.ix[RIDS_to_maintain])\n",
    "        \n",
    "        return rides_with_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping:  houston\n",
      "scraping page: 0\n",
      "scraping page: 1\n",
      "scraping page: 2\n",
      "scraping page: 3\n",
      "original number of rides: 359\n",
      "updated number of rides: 370\n",
      "scraping text from post 0 of 370\n",
      "scraping text from post 10 of 370\n",
      "scraping text from post 20 of 370\n",
      "scraping text from post 30 of 370\n",
      "scraping text from post 40 of 370\n",
      "scraping text from post 50 of 370\n",
      "scraping text from post 60 of 370\n",
      "scraping text from post 70 of 370\n",
      "scraping text from post 80 of 370\n",
      "scraping text from post 90 of 370\n",
      "scraping text from post 100 of 370\n",
      "scraping text from post 110 of 370\n",
      "scraping text from post 120 of 370\n",
      "scraping text from post 130 of 370\n",
      "scraping text from post 140 of 370\n",
      "scraping text from post 150 of 370\n",
      "scraping text from post 160 of 370\n",
      "scraping text from post 170 of 370\n",
      "scraping text from post 180 of 370\n",
      "scraping text from post 190 of 370\n",
      "scraping text from post 200 of 370\n",
      "scraping text from post 210 of 370\n",
      "scraping text from post 220 of 370\n",
      "scraping text from post 230 of 370\n",
      "scraping text from post 240 of 370\n",
      "scraping text from post 250 of 370\n",
      "scraping text from post 260 of 370\n",
      "scraping text from post 270 of 370\n",
      "scraping text from post 280 of 370\n",
      "scraping text from post 290 of 370\n",
      "scraping text from post 300 of 370\n",
      "scraping text from post 310 of 370\n",
      "scraping text from post 320 of 370\n",
      "scraping text from post 330 of 370\n",
      "scraping text from post 340 of 370\n",
      "scraping text from post 350 of 370\n",
      "scraping text from post 360 of 370\n",
      "scraping:  houston\n",
      "scraping page: 0\n",
      "scraping page: 1\n",
      "scraping page: 2\n",
      "scraping page: 3\n",
      "original number of rides: 359\n",
      "updated number of rides: 370\n",
      "scraping text from post 0 of 370\n",
      "scraping text from post 10 of 370\n",
      "scraping text from post 20 of 370\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-49cb3302c8a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu\"df_houston = cityRideshare(US_locations, master_df, url='//houston.craigslist.org/').update_rides()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2262\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2263\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2264\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2265\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m   1039\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0mnumber\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1041\u001b[0;31m         \u001b[0mall_runs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1042\u001b[0m         \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_runs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mquiet\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.10/Frameworks/Python.framework/Versions/2.7/lib/python2.7/timeit.pyc\u001b[0m in \u001b[0;36mrepeat\u001b[0;34m(self, repeat, number)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "\u001b[0;32m<ipython-input-94-b920ee834a8e>\u001b[0m in \u001b[0;36mupdate_rides\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"updated number of rides:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRIDS_to_maintain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mrides_with_body\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scrape_posting_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrides_without_body\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRIDS_to_maintain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrides_with_body\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-94-b920ee834a8e>\u001b[0m in \u001b[0;36m_scrape_posting_body\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup_post\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'section'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'postingbody'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ride_type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup_post\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'p'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'attrgroup'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#avoid crease and desist from Craigslist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df_houston = cityRideshare(US_locations, master_df, url='//houston.craigslist.org/').update_rides()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#todo: scrape every x days but store past data\n",
    "#compare newly scraped RIDs to existing \"active\" posts and update accordingly\n",
    "#three datasets: A) newly scraped RIDs, B) existing RIDs, C) old RIDs\n",
    "#1) only scrape postingbody of A not in B \n",
    "#2) move B not in A to C\n",
    "\n",
    "#todo: use https://www.craigslist.org/about/sites#US to navigate to different cities. \n",
    "#auto-fill options to only show these cities\n",
    "\n",
    "#inherit from master soup class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('houston.pkl','wb') as f:\n",
    "    pickle.dump(df_houston,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
